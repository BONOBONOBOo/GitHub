```
import org.apache.spark.{SparkConf,SparkContext}

object WordCount{
        def main(args:Array[String]){
                require(args.length>=1,
                        "File path")
        val conf = new SparkConf
        val sc = new SparkContext(conf)

        try{
                val filePath=args(0)
                val wordAndCountRDD = sc.textFile(filePath)
                                        .flatMap(_.split("[ ,.]"))
                                        .filter(_.matches("""\p{Alnum}+"""))
                                        .map((_,1))
                                        .reduceByKey(_+_)

                wordAndCountRDD.collect.foreach(println)
        }
        finally{
        sc.stop()
        }
}
}

```

```
spark-submit --master local --class WordCount 
--name WordCount 
~/wordcount-app/target/scala-2.11/spark-simple-app-assembly-0.1.jar  
/data/spark/README.md


```

```scala


def createSalesRDD(csvFile:String)={
      val logRDD = sc.textFile(csvFile)
      logRDD.map{record=>
      val splitRecord = record.split(",")
      val productID = splitRecord(2)
      val numOfSold = splitRecord(3).toInt
      (productID,numOfSold)
      }}

val salesOctRDD = createSalesRDD("/data/spark/sales-october.csv")
val salesNovRDD = createSalesRDD("/data/spark/sale-november.csv")

def createOver50SoldRDD(rdd:RDD[(String,Int)]) = {
    rdd.reduceByKey(_+_).filter(_._2>=50)
}
val octOver50SoldRDD = createOver50SoldRDD(salesOctRDD)
val novOver50SoldRDD = createOver50SoldRDD(salesNovRDD)


val bothOver50SoldRDD = octOver50SoldRDD.join(novOver50SoldRDD)

val over50soldAndAmountRDD = bothOver50SoldRDD.map{
    case(productID,(octAmount,novAmount))=>
    	(productID,octAmount+novAmount)
}
over50soldAndAmountRDD.collect.foreach(println)



```

`





```scala

import scala.collection.mutable.HashMap
import java.io.{BufferedReader,InputStreamReader}
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem,Path}

val productsMap = new HashMap[String,(String,Int)]
val hadoopConf = new Configuration
val fileSystem = FileSystem.get(hadoopConf)

val inputStream = fileSystem.open(new Path("/data/spark/products.csv"))
val productsCSVReader = new BufferedReader(new InputStreamReader(inputStream))
var line = productsCSVReader.readLine

while(line != null){
    val splitLine = line.split(",")
    val productId = splitLine(0)
    val productName = splitLine(1)
    val unitPrice = splitLine(2).toInt
    productsMap(productId) = (productName,unitPrice)
    line = productsCSVReader.readLine
}

productsCSVReader.close()

val broadcastedMap = sc.broadcast(productsMap)

val resultRDD = over50soldAndAmountRDD.map{
    case(productId,amount)=>
    	val productsMap = broadcastedMap.value
    	val (productName,unitPrice) = productsMap(productId)
    	(productName,amount,amount * unitPrice)
}

resultRDD.collect.foreach(println)



```

```scala
import org.apache.spark.{SparkConf,SparkContext}
import org.apache.spark.sql.hive.HiveContext

val conf = new SparkConf()
val sc = new SparkContext(conf)
val sqlContext = new HiveContext(sc)

import sqlContext.implicits._

case class Dessert(menuId:String,name:String,price:Int,kcal:Int)

val dessertRDD = sc.textFile("/data/spark/dessert-menu.csv")

val dessertDF = dessertRDD.map{record =>
	val splitRecord = record.split(",")
	val menuId = splitRecord(0)
    val name = splitRecord(1)
    val price = splitRecord(2).toInt
    val kcal = splitRecord(3).toInt
    Dessert(menuId,name,price,kcal)
}.toDF

dessertDF.printSchema

val rowRDD = dessertDF.rdd

val nameAndPriceRDD = rowRDD.map{row=>
	val name = row.getString(1)
	val price = row.getInt(2)
	(name,price)}

nameAndPriceRDD.collect.foreach(println)



```

```scala
case class Dessert(menuId:String,name:String,price:Int,kcal:Int)

val dessertRDD = sc.textFile("/data/spark/dessert-menu.csv")

val dessertDF = dessertRDD.map{record =>
	val splitRecord = record.split(",")
	val menuId = splitRecord(0)
	val name = splitRecord(1)
	val price = splitRecord(2).toInt
	val kcal = splitRecord(3).toInt
	Dessert(menuId,name,price,kcal)
}.toDF


dessertDF.printSchema

val dfWriter = dessertDF.write
dfWriter.format("parquet").save("/output/dessert-parquet")

val dfReader = spark.read
val dessertDF2 = dfReader.format("parquet").load("/output/dessert-parquet")
dessertDF2.orderBy($"name").show(3)

dessertDF.write.format("parquet").saveAsTable("serssert_fb1_parquet")
spark.read.format("parquet").table("serssert_fb1_parquet").show(3)
#spark.sql("select * from serssert_fb1_parquet limit 3").show



#에러 해결 safemode
scala> import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.SaveMode

 dessertDF.write.format("json").mode(SaveMode.Overwrite).save("/output/dessert_json")

spark.sql("select * from serssert_fb1_parquet order by kcal desc limit 3").show



```

```scala
177p

import java.math.BigDecimal
case class DecimalTypeContainer(data:BigDecimal)

val bdContainerDF = sc.parallelize(
List(new BigDecimal("12345.678888888888888888"))
).map(data=>DecimalTypeContainer(data)).toDF

bdContainerDF.printSchema


bdContainerDF.show(false)

bdContainerDF.write.format("orc").save("/output/bdContainerORC")
val bdContainerORCDF = sqlContext.read.format("orc").load("/output/bdContainerORC")

#val bdContainerORCDF = spark.read.format("orc").load("/output/bdContainerORC")

bdContainerORCDF.printschema

#bdContainerORCDF.orderBy($"data").show(3)


bdContainerDF.show(false)


bdContainerDF.write.format("json").save("/output/bdContainerJSON")



```

```scala
210p

import org.apache.spark.{SparkContext, SparkConf}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.storage.StorageLevel
import org.apache.log4j.{Level, Logger}

Logger.getRootLogger.setLevel(Level.WARN)

val ssc = new StreamingContext(sc, Seconds(10))
val lines = ssc.socketTextStream("localhost", 9999,           StorageLevel.MEMORY_AND_DISK_SER)
val words = lines.flatMap(_.split(" ")).filter(_.nonEmpty)
val wordCounts = words.map((_, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()








```

