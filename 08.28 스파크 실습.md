```
import org.apache.spark.{SparkConf,SparkContext}

object WordCount{
        def main(args:Array[String]){
                require(args.length>=1,
                        "File path")
        val conf = new SparkConf
        val sc = new SparkContext(conf)

        try{
                val filePath=args(0)
                val wordAndCountRDD = sc.textFile(filePath)
                                        .flatMap(_.split("[ ,.]"))
                                        .filter(_.matches("""\p{Alnum}+"""))
                                        .map((_,1))
                                        .reduceByKey(_+_)

                wordAndCountRDD.collect.foreach(println)
        }
        finally{
        sc.stop()
        }
}
}

```

```
spark-submit --master local --class WordCount 
--name WordCount 
~/wordcount-app/target/scala-2.11/spark-simple-app-assembly-0.1.jar  
/data/spark/README.md


```

```scala


def createSalesRDD(csvFile:String)={
      val logRDD = sc.textFile(csvFile)
      logRDD.map{record=>
      val splitRecord = record.split(",")
      val productID = splitRecord(2)
      val numOfSold = splitRecord(3).toInt
      (productID,numOfSold)
      }}

val salesOctRDD = createSalesRDD("/data/spark/sales-october.csv")
val salesNovRDD = createSalesRDD("/data/spark/sale-november.csv")

def createOver50SoldRDD(rdd:RDD[(String,Int)]) = {
    rdd.reduceByKey(_+_).filter(_._2>=50)
}
val octOver50SoldRDD = createOver50SoldRDD(salesOctRDD)
val novOver50SoldRDD = createOver50SoldRDD(salesNovRDD)


val bothOver50SoldRDD = octOver50SoldRDD.join(novOver50SoldRDD)

val over50soldAndAmountRDD = bothOver50SoldRDD.map{
    case(productID,(octAmount,novAmount))=>
    	(productID,octAmount+novAmount)
}
over50soldAndAmountRDD.collect.foreach(println)



```

`





```scala

import scala.collection.mutable.HashMap
import java.io.{BufferedReader,InputStreamReader}
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem,Path}

val productsMap = new HashMap[String,(String,Int)]
val hadoopConf = new Configuration
val fileSystem = FileSystem.get(hadoopConf)

val inputStream = fileSystem.open(new Path("/data/spark/products.csv"))
val productsCSVReader = new BufferedReader(new InputStreamReader(inputStream))
var line = productsCSVReader.readLine

while(line != null){
    val splitLine = line.split(",")
    val productId = splitLine(0)
    val productName = splitLine(1)
    val unitPrice = splitLine(2).toInt
    productsMap(productId) = (productName,unitPrice)
    line = productsCSVReader.readLine
}

productsCSVReader.close()

val broadcastedMap = sc.broadcast(productsMap)

val resultRDD = over50soldAndAmountRDD.map{
    case(productId,amount)=>
    	val productsMap = broadcastedMap.value
    	val (productName,unitPrice) = productsMap(productId)
    	(productName,amount,amount * unitPrice)
}

resultRDD.collect.foreach(println)



```

```scala
import org.apache.spark.{SparkConf,SparkContext}
import org.apache.spark.sql.hive.HiveContext

val conf = new SparkConf()
val sc = new SparkContext(conf)
val sqlContext = new HiveContext(sc)

import sqlContext.implicits._

case class Dessert(menuId:String,name:String,price:Int,kcal:Int)

val dessertRDD = sc.textFile("/data/spark/dessert-menu.csv")

val dessertDF = dessertRDD.map{record =>
	val splitRecord = record.split(",")
	val menuId = splitRecord(0)
    val name = splitRecord(1)
    val price = splitRecord(2).toInt
    val kcal = splitRecord(3).toInt
    Dessert(menuId,name,price,kcal)
}.toDF

dessertDF.printSchema

val rowRDD = dessertDF.rdd

val nameAndPriceRDD = rowRDD.map{row=>
	val name = row.getString(1)
	val price = row.getInt(2)
	(name,price)}

nameAndPriceRDD.collect.foreach(println)



```

